## 动手学深度学习

### 3月20日

#### 数据预处理

```
https://zh-v2.d2l.ai/chapter_preliminaries/pandas.html
```

处理缺失值：将字符串变成数值，缺失值部分均值化处理。输入为张量 `torch.tensor(inputs.values)`。

### 3月21日

#### 线性代数

1. 标量的计算、向量创造如 `arrange`等、`arrange`之后使用 `reshape`来塑造成矩阵
2. 矩阵转置 `B.T`、矩阵的 `B=A.clone()`是分配了新的内存
3. 按元素乘 `A*B`、所有元素求和 `A.sum()`
4. If `A.shape = [2,4,5]`, then `A.sum(axis=0).shape = [4,5]`, `A.sum(axis = 1).shape = [2,5]`
5. matrix multiplication : `torch.mv(A,b)`
6. L_2 的范数 = `torch.norm()`, L_1 = `torch.abs(A).sum()`

#### 矩阵运算

1. 矩阵导数的形状比较重要

#### 自动求导

1. 符号求导和数值求导

2. > 1. 将代码分解为操作子
   >
   > 2. 将计算表示成一个无环图
   >
   >    <img src="https://raw.githubusercontent.com/yuukireina05/picture-repository/main/image-20210321140508731.png" alt="image-20210321140508731" style="zoom:25%;" />
   >
   > 3. 显示构造
   >
   >    ```
   >    from mxnet import sym
   >    a = sym.var()
   >    b = sym.var()
   >    c - 2*a + b
   >    ```
   >
   > 4. 隐式构造
   >
   > 5. 反向积累

3. 实现

   ```python
   import torch 
   x = 
   x.requires_grad_(Ture)
   ```

4. <img src="https://raw.githubusercontent.com/yuukireina05/picture-repository/main/image-20210321141650990.png" alt="image-20210321141650990" style="zoom:50%;" />

5. 累积梯度：对于批量-128，一次性算不下的情况，可以切成4下，一次64下，然后再累计梯度

#### 查阅文档

1. 

### 3月28日

#### Softmax

问答环节：

1. 能提一下soflabel训练策略吗？以及为什么有效
2. Is it the same between softmax regression and logistic regression, if they are different, where?
3. why use cross entrophy

### 图像分类

- 导入数据集，确定数据集的图片数据格式
- 构造数据集后，将其放进一个dataloader里
- 对图片reshape成batches 加channels 和长宽的格式
- resize是当图片数据格式发生变化时，可以重新调整
- Softmax输入需要是一个向量，所以需要展平图片，变成一个向量。输出十个类，因此输出为10
- torch.matmul
- 超参数
- accuracy
- new.eval()设置为评估模式，可以不更新梯度之类的，就节省性能

### 4月10日

#### 感知机: 感知机接收多个输入信号，输出一个信号。

##### 训练感知机:

- 收敛定理，存在$\rho$使得$y(\mathbf{x^Tw}+b)\geq \rho$，感知机在$\frac{r^2+1}{\rho^2}$步后收敛
- 感知机无法拟合XOR函数，只能产生线性分割面

##### 多层感知机：多层简单函数来做更复杂的判断

- 可以学习XOR函数

  <img src="https://raw.githubusercontent.com/yuukireina05/picture-repository/main/image-20210410132553328.png" alt="image-20210410132553328" style="zoom:33%;" />

- $\sigma$为按元素的**激活函数**，如sigmoid函数，是引入非线性因素的关键。需满足下述几个条件：

  - 连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以直接利用数值优化的方法来学习网络参数。
  - 激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。
  - 激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。 

- 隐藏层$\mathbf{h_1}=\sigma(\mathbf{W_1x}+\mathbf{b_1})$ ，输出层$\mathbf{o}=\mathbf{W}_{4} \mathbf{h}_{3}+\mathbf{b}_{4}$ 

- 输入维度高，输出分类少，那推荐是设置金字塔型隐藏层以便逐渐收缩维度

- 多层感知机的[pytorch实现](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html)

  > 1. 初始化模型参数
  > 2. 定义激活函数
  > 3. 搭建模型
  > 4. 定义损失函数
  > 5. 开始训练

- 



